"""
üéØ –®–ü–ê–†–ì–ê–õ–ö–ê: –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è HH Parser —Å –Ω–æ–≤—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
"""

from hh_parser import HHParser

# ================================================================
# üî• –ü–†–ò–ú–ï–† 1: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∞–≤—Ç–æ–æ—Ç–∫–ª–∏–∫–∞
# ================================================================

def example_autorespond():
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è –∞–≤—Ç–æ–æ—Ç–∫–ª–∏–∫–∞"""
    parser = HHParser(delay=0.3)
    
    vacancies = parser.search_vacancies(
        keywords="–≤—Ö–æ–¥—è—â–∏–µ –∑–∞—è–≤–∫–∏ CRM –æ–ø–µ—Ä–∞—Ç–æ—Ä –º–µ–Ω–µ–¥–∂–µ—Ä",
        area=1,                           # –ú–æ—Å–∫–≤–∞
        salary=50000,                     # –û—Ç 50–∫
        only_with_salary=True,            # –¢–æ–ª—å–∫–æ —Å –∑–∞—Ä–ø–ª–∞—Ç–æ–π ‚úÖ
        period=7,                         # –ó–∞ –Ω–µ–¥–µ–ª—é ‚úÖ
        excluded_text="–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç",  # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä ‚úÖ
        order_by='publication_time',      # –ù–æ–≤—ã–µ –ø–µ—Ä–≤—ã–µ ‚úÖ
        max_pages=15                      # 1500 –≤–∞–∫–∞–Ω—Å–∏–π
    )
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ: {len(vacancies)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    parser.save_to_json(vacancies, 'vacancies_filtered.json')
    
    return vacancies


# ================================================================
# üéØ –ü–†–ò–ú–ï–† 2: –£–¥–∞–ª–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –ø–æ –≤—Å–µ–π –†–æ—Å—Å–∏–∏
# ================================================================

def example_remote_work():
    """–ü–æ–∏—Å–∫ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π"""
    parser = HHParser()
    
    vacancies = parser.search_vacancies(
        keywords="–æ–ø–µ—Ä–∞—Ç–æ—Ä CRM –≤—Ö–æ–¥—è—â–∏–µ",
        area=113,                         # –í—Å—è –†–æ—Å—Å–∏—è
        salary=40000,                     # –û—Ç 40–∫
        only_with_salary=True,
        period=7,
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å schedule='remote' –≤ –±—É–¥—É—â–µ–º
        excluded_text="–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä –ø—Ä–æ–¥–∞–∂–∏",
        order_by='publication_time',
        max_pages=10
    )
    
    return vacancies


# ================================================================
# üíé –ü–†–ò–ú–ï–† 3: –í—ã—Å–æ–∫–æ–æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
# ================================================================

def example_high_salary():
    """–¢–æ–ø–æ–≤—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏ —Å –≤—ã—Å–æ–∫–æ–π –∑–∞—Ä–ø–ª–∞—Ç–æ–π"""
    parser = HHParser()
    
    vacancies = parser.search_vacancies(
        keywords="–º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ —Ä–∞–±–æ—Ç–µ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏",
        area=1,                           # –ú–æ—Å–∫–≤–∞
        salary=100000,                    # –û—Ç 100–∫
        only_with_salary=True,
        period=30,                        # –ú–µ—Å—è—Ü (—Ç–∞–∫ –∫–∞–∫ –≤—ã—Å–æ–∫–∞—è –∑–∞—Ä–ø–ª–∞—Ç–∞ = –º–µ–Ω—å—à–µ –≤–∞–∫–∞–Ω—Å–∏–π)
        order_by='salary_desc',           # –°–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ –≤—ã—Å–æ–∫–æ–æ–ø–ª–∞—á–∏–≤–∞–µ–º—ã–µ
        max_pages=5
    )
    
    return vacancies


# ================================================================
# üîç –ü–†–ò–ú–ï–† 4: –®–∏—Ä–æ–∫–∏–π –ø–æ–∏—Å–∫ (–º–∞–∫—Å–∏–º—É–º –≤–∞–∫–∞–Ω—Å–∏–π)
# ================================================================

def example_wide_search():
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞–∫–∞–Ω—Å–∏–π –±–µ–∑ —Å—Ç—Ä–æ–≥–∏—Ö —Ñ–∏–ª—å—Ç—Ä–æ–≤"""
    parser = HHParser()
    
    vacancies = parser.search_vacancies(
        keywords="–º–µ–Ω–µ–¥–∂–µ—Ä –∫–ª–∏–µ–Ω—Ç",
        area=1,
        salary=None,                      # –ë–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –∑–∞—Ä–ø–ª–∞—Ç—ã
        only_with_salary=False,           # –í—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏
        period=30,                        # –ó–∞ –º–µ—Å—è—Ü
        excluded_text="–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å",     # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        order_by='relevance',             # –ü–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        max_pages=50                      # –ú–Ω–æ–≥–æ —Å—Ç—Ä–∞–Ω–∏—Ü
    )
    
    return vacancies


# ================================================================
# ‚ö° –ü–†–ò–ú–ï–† 5: –°—É–ø–µ—Ä-–±—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ (—Å–≤–µ–∂–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏)
# ================================================================

def example_fresh_only():
    """–¢–æ–ª—å–∫–æ –≤—á–µ—Ä–∞—à–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–∫–ª–∏–∫–∞"""
    parser = HHParser(delay=0.2)  # –ú–µ–Ω—å—à–µ –∑–∞–¥–µ—Ä–∂–∫–∞
    
    vacancies = parser.search_vacancies(
        keywords="–æ–ø–µ—Ä–∞—Ç–æ—Ä –≤—Ö–æ–¥—è—â–∏–µ CRM",
        area=1,
        salary=45000,
        only_with_salary=True,
        period=1,                         # ‚ö° –¢–û–õ–¨–ö–û –ó–ê –í–ß–ï–†–ê!
        excluded_text="–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ",
        order_by='publication_time',
        max_pages=5                       # –ú–µ–Ω—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü = –±—ã—Å—Ç—Ä–µ–µ
    )
    
    print(f"‚ö° –°–≤–µ–∂–∞–∫! –ù–∞–π–¥–µ–Ω–æ {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å")
    
    return vacancies


# ================================================================
# üé® –ü–†–ò–ú–ï–† 6: –î–ª—è API endpoint (FastAPI)
# ================================================================

def example_api_usage():
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ API"""
    from fastapi import FastAPI
    from pydantic import BaseModel
    from typing import Optional
    
    app = FastAPI()
    
    class VacancySearchRequest(BaseModel):
        keywords: str
        region: int = 1
        min_salary: Optional[int] = 50000
        only_with_salary: bool = True
        days: int = 7
        excluded_words: str = "–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä —Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ"
        max_results: int = 1000  # –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    @app.post("/api/search-vacancies")
    async def search_vacancies(request: VacancySearchRequest):
        parser = HHParser()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º max_results –≤ max_pages (100 –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É)
        max_pages = (request.max_results + 99) // 100
        
        try:
            vacancies = parser.search_vacancies(
                keywords=request.keywords,
                area=request.region,
                salary=request.min_salary,
                only_with_salary=request.only_with_salary,
                period=request.days,
                excluded_text=request.excluded_words,
                order_by='publication_time',
                max_pages=max_pages
            )
            
            return {
                "success": True,
                "count": len(vacancies),
                "vacancies": vacancies[:request.max_results]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–æ –∑–∞–ø—Ä–æ—Å—É
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }


# ================================================================
# üìä –ü–†–ò–ú–ï–† 7: –° –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
# ================================================================

def example_with_statistics():
    """–ü–∞—Ä—Å–∏–Ω–≥ + –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    parser = HHParser()
    
    vacancies = parser.search_vacancies(
        keywords="–º–µ–Ω–µ–¥–∂–µ—Ä CRM",
        area=1,
        salary=50000,
        only_with_salary=True,
        period=7,
        excluded_text="–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä",
        order_by='publication_time',
        max_pages=10
    )
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if vacancies:
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞—Ä–ø–ª–∞—Ç–∞–º
        salaries = []
        for v in vacancies:
            salary_text = v['–æ–ø–ª–∞—Ç–∞']
            if '–æ—Ç' in salary_text:
                try:
                    salary = int(salary_text.split('–æ—Ç')[1].split()[0].replace(' ', ''))
                    salaries.append(salary)
                except:
                    pass
        
        if salaries:
            avg_salary = sum(salaries) / len(salaries)
            print(f"\nüí∞ –°—Ä–µ–¥–Ω—è—è –∑–∞—Ä–ø–ª–∞—Ç–∞: {avg_salary:,.0f} —Ä—É–±.".replace(',', ' '))
            print(f"üí∞ –ú–∏–Ω–∏–º—É–º: {min(salaries):,} —Ä—É–±.".replace(',', ' '))
            print(f"üí∞ –ú–∞–∫—Å–∏–º—É–º: {max(salaries):,} —Ä—É–±.".replace(',', ' '))
        
        # –¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–π
        from collections import Counter
        companies = [v['–∫–æ–º–ø–∞–Ω–∏—è'] for v in vacancies]
        top_companies = Counter(companies).most_common(5)
        
        print("\nüè¢ –¢–æ–ø-5 –∫–æ–º–ø–∞–Ω–∏–π –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –≤–∞–∫–∞–Ω—Å–∏–π:")
        for company, count in top_companies:
            print(f"  ‚Ä¢ {company}: {count} –≤–∞–∫–∞–Ω—Å–∏–π")
    
    return vacancies


# ================================================================
# üîÑ –ü–†–ò–ú–ï–† 8: –ë–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ (–Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤)
# ================================================================

def example_batch_processing():
    """–ù–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ä–∞–∑–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
    parser = HHParser()
    
    all_vacancies = []
    
    # –ó–∞–ø—Ä–æ—Å 1: –û–ø–µ—Ä–∞—Ç–æ—Ä—ã
    print("\n1Ô∏è‚É£ –ò—â—É –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤...")
    vacancies_1 = parser.search_vacancies(
        keywords="–æ–ø–µ—Ä–∞—Ç–æ—Ä –≤—Ö–æ–¥—è—â–∏–µ CRM",
        area=1,
        salary=40000,
        only_with_salary=True,
        period=7,
        excluded_text="–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å",
        order_by='publication_time',
        max_pages=5
    )
    all_vacancies.extend(vacancies_1)
    
    # –ó–∞–ø—Ä–æ—Å 2: –ú–µ–Ω–µ–¥–∂–µ—Ä—ã
    print("\n2Ô∏è‚É£ –ò—â—É –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤...")
    vacancies_2 = parser.search_vacancies(
        keywords="–º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ —Ä–∞–±–æ—Ç–µ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏ CRM",
        area=1,
        salary=50000,
        only_with_salary=True,
        period=7,
        excluded_text="–Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –±—Ä–æ–∫–µ—Ä",
        order_by='publication_time',
        max_pages=5
    )
    all_vacancies.extend(vacancies_2)
    
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ ID
    unique_vacancies = {v['id']: v for v in all_vacancies}.values()
    
    print(f"\n‚úÖ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(all_vacancies)}")
    print(f"‚úÖ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique_vacancies)}")
    
    return list(unique_vacancies)


# ================================================================
# üöÄ –ó–ê–ü–£–°–ö –ü–†–ò–ú–ï–†–û–í
# ================================================================

if __name__ == "__main__":
    print("="*60)
    print("üéØ –ü–†–ò–ú–ï–†–´ –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø HH PARSER")
    print("="*60)
    
    # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –Ω—É–∂–Ω—ã–π –ø—Ä–∏–º–µ—Ä:
    
    # vacancies = example_autorespond()           # –ü—Ä–∏–º–µ—Ä 1
    # vacancies = example_remote_work()           # –ü—Ä–∏–º–µ—Ä 2
    # vacancies = example_high_salary()           # –ü—Ä–∏–º–µ—Ä 3
    # vacancies = example_wide_search()           # –ü—Ä–∏–º–µ—Ä 4
    vacancies = example_fresh_only()              # –ü—Ä–∏–º–µ—Ä 5 ‚ö°
    # vacancies = example_with_statistics()       # –ü—Ä–∏–º–µ—Ä 7
    # vacancies = example_batch_processing()      # –ü—Ä–∏–º–µ—Ä 8
    
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ù–∞–π–¥–µ–Ω–æ {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")


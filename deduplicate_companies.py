"""
–ö–ê–ß–ï–°–¢–í–ï–ù–ù–û–ï –£–î–ê–õ–ï–ù–ò–ï –î–£–ë–õ–ò–ö–ê–¢–û–í –ü–û –ö–û–ú–ü–ê–ù–ò–Ø–ú
–û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ 1 –≤–∞–∫–∞–Ω—Å–∏—é –æ—Ç –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ (–ª—É—á—à—É—é)
"""

import json
from pathlib import Path
from typing import List, Dict, Tuple
from collections import defaultdict


class CompanyDeduplicator:
    """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤–∞–∫–∞–Ω—Å–∏–π –æ—Ç –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏"""
    
    def __init__(self):
        self.stats = {
            'total_vacancies': 0,
            'unique_companies': 0,
            'duplicates_removed': 0,
            'kept_vacancies': 0
        }
        
        # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π –≤–∞–∫–∞–Ω—Å–∏–∏
        self.priority_keywords = [
            '–≤—Ö–æ–¥—è—â–∏–µ –∑–∞—è–≤–∫–∏',
            '–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞—è–≤–æ–∫',
            'crm',
            '–±–∏—Ç—Ä–∏–∫—Å',
            'amoCRM',
            '—á–∞—Ç',
            '–æ–ø–µ—Ä–∞—Ç–æ—Ä',
            '–º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ —Ä–∞–±–æ—Ç–µ —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏',
            'support',
            '—Ç–µ—Ö–ø–æ–¥–¥–µ—Ä–∂–∫–∞',
            '–∫–æ–ª–ª-—Ü–µ–Ω—Ç—Ä'
        ]
    
    def normalize_company_name(self, company: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        """
        if not company:
            return ""
        
        company_lower = company.lower().strip()
        
        # –£–±–∏—Ä–∞–µ–º –æ–±—â–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã/—Å—É—Ñ—Ñ–∏–∫—Å—ã
        replacements = [
            ('–æ–æ–æ ', ''),
            ('–æ–∞–æ ', ''),
            ('–∑–∞–æ ', ''),
            ('–ø–∞–æ ', ''),
            ('–∏–ø ', ''),
            ('–∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–π –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å ', ''),
            (' –æ–æ–æ', ''),
            (' –æ–∞–æ', ''),
            ('"', ''),
            ("'", ''),
            ('¬´', ''),
            ('¬ª', ''),
        ]
        
        for old, new in replacements:
            company_lower = company_lower.replace(old, new)
        
        return company_lower.strip()
    
    def calculate_vacancy_score(self, vacancy: Dict) -> int:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤–∞–∫–∞–Ω—Å–∏–∏ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π
        –ß–µ–º –≤—ã—à–µ –æ—Ü–µ–Ω–∫–∞, —Ç–µ–º –ª—É—á—à–µ –≤–∞–∫–∞–Ω—Å–∏—è
        """
        score = 0
        
        title = vacancy.get('–Ω–∞–∑–≤–∞–Ω–∏–µ', '').lower()
        description = vacancy.get('–æ–ø–∏—Å–∞–Ω–∏–µ', '').lower()
        
        # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –∏–∑ pre_score –µ—Å–ª–∏ –µ—Å—Ç—å
        if '_pre_score' in vacancy:
            score += vacancy['_pre_score'] * 10
        else:
            score += 50  # –±–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (–±–æ–ª—å—à–µ –≤–µ—Å)
        for keyword in self.priority_keywords:
            if keyword.lower() in title:
                score += 20
        
        # –ë–æ–Ω—É—Å—ã –∑–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
        for keyword in self.priority_keywords:
            if keyword.lower() in description:
                score += 5
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã
        salary = vacancy.get('–æ–ø–ª–∞—Ç–∞', '')
        if salary and salary != '–ù–µ —É–∫–∞–∑–∞–Ω–∞' and '—Ä—É–±' in salary:
            score += 10
        
        # –ë–æ–Ω—É—Å –∑–∞ –¥–ª–∏–Ω—É –æ–ø–∏—Å–∞–Ω–∏—è (–±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã–µ –æ–ø–∏—Å–∞–Ω–∏—è)
        desc_length = len(description)
        if desc_length > 1000:
            score += 10
        elif desc_length > 500:
            score += 5
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–µ–¥–∞–≤–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏
        date_pub = vacancy.get('–¥–∞—Ç–∞_–ø—É–±–ª–∏–∫–∞—Ü–∏–∏', '')
        if '2025-12' in date_pub:  # –î–µ–∫–∞–±—Ä—å 2025
            score += 15
        elif '2025-11' in date_pub:  # –ù–æ—è–±—Ä—å 2025
            score += 10
        
        return score
    
    def select_best_vacancy(self, vacancies: List[Dict]) -> Dict:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –≤–∞–∫–∞–Ω—Å–∏—é –∏–∑ —Å–ø–∏—Å–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        if len(vacancies) == 1:
            return vacancies[0]
        
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –≤—Å–µ –≤–∞–∫–∞–Ω—Å–∏–∏
        scored_vacancies = [
            (vacancy, self.calculate_vacancy_score(vacancy))
            for vacancy in vacancies
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
        scored_vacancies.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à—É—é
        best_vacancy = scored_vacancies[0][0]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥—É–±–ª–∏–∫–∞—Ç–∞—Ö
        best_vacancy['_duplicates_count'] = len(vacancies) - 1
        best_vacancy['_dedup_score'] = scored_vacancies[0][1]
        
        return best_vacancy
    
    def deduplicate_batch(self, vacancies: List[Dict]) -> Tuple[List[Dict], List[Dict]]:
        """
        –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ –±–∞—Ç—á–µ
        
        Returns:
            (kept_vacancies, removed_duplicates)
        """
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º
        companies = defaultdict(list)
        
        for vacancy in vacancies:
            company = vacancy.get('–∫–æ–º–ø–∞–Ω–∏—è', '')
            if not company:
                # –í–∞–∫–∞–Ω—Å–∏–∏ –±–µ–∑ –∫–æ–º–ø–∞–Ω–∏–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                companies['_no_company_' + str(id(vacancy))].append(vacancy)
            else:
                normalized = self.normalize_company_name(company)
                companies[normalized].append(vacancy)
        
        kept = []
        removed = []
        
        for company_key, company_vacancies in companies.items():
            if len(company_vacancies) == 1:
                # –ù–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                kept.append(company_vacancies[0])
            else:
                # –ï—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã - –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é
                best = self.select_best_vacancy(company_vacancies)
                kept.append(best)
                
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –¥—É–±–ª–∏–∫–∞—Ç—ã
                for vac in company_vacancies:
                    if vac != best:
                        removed.append({
                            'vacancy': vac,
                            'reason': f"–î—É–±–ª–∏–∫–∞—Ç –∫–æ–º–ø–∞–Ω–∏–∏ '{vac.get('–∫–æ–º–ø–∞–Ω–∏—è', '')}' (–æ—Å—Ç–∞–≤–ª–µ–Ω–∞ –ª—É—á—à–∞—è)",
                            'kept_vacancy_id': best.get('id', ''),
                            'kept_vacancy_title': best.get('–Ω–∞–∑–≤–∞–Ω–∏–µ', '')
                        })
        
        return kept, removed
    
    def process_directory(self, input_dir: str, output_dir: str = None):
        """
        –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö filtered_batch —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        
        Args:
            input_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å filtered_batch —Ñ–∞–π–ª–∞–º–∏
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–µ—Å–ª–∏ None, –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º)
        """
        input_path = Path(input_dir)
        
        if output_dir:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)
        else:
            output_path = input_path
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ filtered_batch —Ñ–∞–π–ª—ã
        filtered_files = sorted(input_path.glob('filtered_batch_*.json'))
        
        if not filtered_files:
            print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ filtered_batch_*.json –≤ {input_dir}")
            return
        
        print("=" * 70)
        print("üîÑ –£–î–ê–õ–ï–ù–ò–ï –î–£–ë–õ–ò–ö–ê–¢–û–í –ü–û –ö–û–ú–ü–ê–ù–ò–Ø–ú")
        print("=" * 70)
        print()
        print(f"üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(filtered_files)}")
        print()
        print("üìã –ü—Ä–∞–≤–∏–ª–∞ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏:")
        print("   1. –û–¥–Ω–∞ –∫–æ–º–ø–∞–Ω–∏—è = –æ–¥–Ω–∞ –≤–∞–∫–∞–Ω—Å–∏—è")
        print("   2. –í—ã–±–∏—Ä–∞–µ—Ç—Å—è –ª—É—á—à–∞—è –≤–∞–∫–∞–Ω—Å–∏—è –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º:")
        print("      - –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–≤—Ö–æ–¥—è—â–∏–µ –∑–∞—è–≤–∫–∏, CRM)")
        print("      - –ù–∞–ª–∏—á–∏–µ –∑–∞—Ä–ø–ª–∞—Ç—ã")
        print("      - –°–≤–µ–∂–µ—Å—Ç—å –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
        print("      - –ü–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è")
        print()
        
        # –¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–≤–æ–º —Ñ–∞–π–ª–µ
        print("üß™ –¢–µ—Å—Ç–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–≤–æ–º –±–∞—Ç—á–µ...")
        test_file = filtered_files[0]
        
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        test_kept, test_removed = self.deduplicate_batch(test_data)
        
        print(f"   –§–∞–π–ª: {test_file.name}")
        print(f"   –ë—ã–ª–æ: {len(test_data)} –≤–∞–∫–∞–Ω—Å–∏–π")
        print(f"   –°—Ç–∞–ª–æ: {len(test_kept)} –≤–∞–∫–∞–Ω—Å–∏–π")
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(test_removed)}")
        
        if test_removed:
            print()
            print("   –ü—Ä–∏–º–µ—Ä—ã —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤:")
            for i, dup in enumerate(test_removed[:3], 1):
                vac = dup['vacancy']
                print(f"   {i}. {vac.get('–∫–æ–º–ø–∞–Ω–∏—è', 'N/A')} - {vac.get('–Ω–∞–∑–≤–∞–Ω–∏–µ', 'N/A')[:60]}...")
        
        print()
        response = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤? (–¥–∞/–Ω–µ—Ç): ").strip().lower()
        if response not in ['–¥–∞', 'yes', 'y', '–¥']:
            print("–û—Ç–º–µ–Ω–µ–Ω–æ.")
            return
        
        print()
        print("üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤...")
        print()
        
        all_removed = []
        
        for i, file_path in enumerate(filtered_files, 1):
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
            except Exception as e:
                print(f"‚ö†Ô∏è [{i}/{len(filtered_files)}] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path.name}: {e}")
                continue
            
            # –î–µ–¥—É–ø–ª–∏—Ü–∏—Ä—É–µ–º
            kept, removed = self.deduplicate_batch(batch_data)
            
            self.stats['total_vacancies'] += len(batch_data)
            self.stats['kept_vacancies'] += len(kept)
            self.stats['duplicates_removed'] += len(removed)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ–¥—É–ø–ª–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –±–∞—Ç—á
            output_file = output_path / file_path.name
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(kept, f, ensure_ascii=False, indent=2)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö
            all_removed.extend(removed)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            if len(removed) > 0:
                print(f"[{i}/{len(filtered_files)}] {file_path.name}: –±—ã–ª–æ {len(batch_data)}, —É–±—Ä–∞–ª–∏ {len(removed)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, –æ—Å—Ç–∞–ª–æ—Å—å {len(kept)}")
            else:
                print(f"[{i}/{len(filtered_files)}] {file_path.name}: {len(batch_data)} (–±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)", end='\r')
        
        print()
        print()
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
        all_companies = set()
        for file_path in filtered_files:
            with open(output_path / file_path.name, 'r', encoding='utf-8') as f:
                data = json.load(f)
                for vac in data:
                    company = vac.get('–∫–æ–º–ø–∞–Ω–∏—è', '')
                    if company:
                        all_companies.add(self.normalize_company_name(company))
        
        self.stats['unique_companies'] = len(all_companies)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —É–¥–∞–ª–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
        if all_removed:
            removed_file = output_path / 'removed_duplicates.json'
            with open(removed_file, 'w', encoding='utf-8') as f:
                json.dump(all_removed, f, ensure_ascii=False, indent=2)
            print(f"üíæ –£–¥–∞–ª–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {removed_file}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_file = output_path / 'filtering_stats.json'
        if stats_file.exists():
            with open(stats_file, 'r', encoding='utf-8') as f:
                old_stats = json.load(f)
            
            new_stats = {
                'total_batches': old_stats.get('total_batches', 0),
                'total_vacancies': self.stats['kept_vacancies'],
                'total_excluded': old_stats.get('total_excluded', 0) + self.stats['duplicates_removed'],
                'total_to_process': self.stats['kept_vacancies'],
                'unique_companies': self.stats['unique_companies'],
                'duplicates_removed': self.stats['duplicates_removed'],
                'total_high_priority': old_stats.get('total_high_priority', 0),
                'total_medium_priority': old_stats.get('total_medium_priority', 0),
                'total_low_priority': old_stats.get('total_low_priority', 0)
            }
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(new_stats, f, ensure_ascii=False, indent=2)
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print()
        print("=" * 70)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–î–£–ü–õ–ò–ö–ê–¶–ò–ò")
        print("=" * 70)
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤–∞–∫–∞–Ω—Å–∏–π: {self.stats['total_vacancies']}")
        print(f"–£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates_removed']} ({self.stats['duplicates_removed']/self.stats['total_vacancies']*100:.1f}%)")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å –≤–∞–∫–∞–Ω—Å–∏–π: {self.stats['kept_vacancies']}")
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {self.stats['unique_companies']}")
        print()
        print(f"–°—Ä–µ–¥–Ω–µ–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏—é: {self.stats['total_vacancies']/self.stats['unique_companies']:.2f}")
        print("=" * 70)
        print()
        print("‚úÖ –ì–û–¢–û–í–û!")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –±–∞—Ç—á–∞–º–∏
    INPUT_DIR = "filtered_batches"
    
    # –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã–≤–æ–¥–∞ –∏–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å None –¥–ª—è –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∏
    OUTPUT_DIR = None  # None = –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º —Ç–µ –∂–µ —Ñ–∞–π–ª—ã
    
    # –°–æ–∑–¥–∞–µ–º –¥–µ–¥—É–ø–ª–∏–∫–∞—Ç–æ—Ä
    deduplicator = CompanyDeduplicator()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
    deduplicator.process_directory(INPUT_DIR, OUTPUT_DIR)
    
    print()
    print("üí° –°–æ–≤–µ—Ç: –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–∞–π–ª 'removed_duplicates.json' - —Ç–∞–º –≤—Å–µ —É–¥–∞–ª–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã")


if __name__ == "__main__":
    main()

